{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPopZ3NL5Q8k+Zk7/x8Yzmw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/georginobaltazar/Trabalhos-TP558/blob/main/Semin%C3%A1rio%2002/Exeplo_aplic_Semi2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automatic Speech Recognition with Transformer\n",
        "Author: Apoorv Nandan\n",
        "Date created: 2021/01/13\n",
        "Last modified: 2021/01/13\n",
        "Description: Training a sequence-to-sequence Transformer for automatic speech recognition.\n",
        "\n",
        "Introduction\n",
        "O reconhecimento automático de fala (ASR) consiste na transcrição de segmentos de fala de áudio em texto. ASR pode ser tratado como um problema de sequência a sequência, onde o áudio pode ser representado como uma sequência de vetores de recursos e o texto como uma sequência de caracteres, palavras ou tokens de subpalavras.\n",
        "\n",
        "Para esta demonstração, usaremos o conjunto de dados LJSpeech do projeto LibriVox. Consiste em pequenos clipes de áudio de um único palestrante lendo passagens de 7 livros de não ficção. Nosso modelo será semelhante ao Transformer original (codificador e decodificador), conforme proposto no artigo \"Atenção é tudo que você precisa\"\n",
        "\n",
        "References:\n",
        "\n",
        "Attention is All You Need\n",
        "Very Deep Self-Attention Networks for End-to-End Speech Recognition\n",
        "Speech Transformers\n",
        "LJSpeech Dataset"
      ],
      "metadata": {
        "id": "CRnnNaMU0lpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código a seguir implementa um modelo de reconhecimento automático de fala (ASR) usando a arquitetura Transformer em TensorFlow e Keras. Este exemplo inclui o carregamento dos dados, definição do modelo, treinamento e avaliação"
      ],
      "metadata": {
        "id": "UHqWS26bPXpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importações e Configurações Iniciais"
      ],
      "metadata": {
        "id": "0-KktE7_bg_8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiXG1xWtPMBJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "# Configurar o backend do Keras para TensorFlow\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definição das Camadas de Embedding\n",
        "\n",
        "Token Embedding: Utilizado para mapear tokens (caracteres ou palavras) em vetores de embedding, incluindo codificação posicional"
      ],
      "metadata": {
        "id": "SO5aQ33Nboq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(layers.Layer):\n",
        "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n",
        "        super().__init__()\n",
        "        self.emb = keras.layers.Embedding(num_vocab, num_hid)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        x = self.emb(x)\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        return x + positions\n"
      ],
      "metadata": {
        "id": "AThkE97Vbq9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speech Feature Embedding: Extrai características do áudio utilizando camadas de convolução."
      ],
      "metadata": {
        "id": "KkuNw-VHb08b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeechFeatureEmbedding(layers.Layer):\n",
        "    def __init__(self, num_hid=64, maxlen=100):\n",
        "        super().__init__()\n",
        "        self.conv1 = keras.layers.Conv1D(num_hid, 11, strides=2, padding=\"same\", activation=\"relu\")\n",
        "        self.conv2 = keras.layers.Conv1D(num_hid, 11, strides=2, padding=\"same\", activation=\"relu\")\n",
        "        self.conv3 = keras.layers.Conv1D(num_hid, 11, strides=2, padding=\"same\", activation=\"relu\")\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return self.conv3(x)\n"
      ],
      "metadata": {
        "id": "-CYTiRd4b4On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definição do Transformer Encoder"
      ],
      "metadata": {
        "id": "kDqmW5-Db8aK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential([\n",
        "            layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ],
      "metadata": {
        "id": "yaAsAhd9b9rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definição do Transformer Decoder"
      ],
      "metadata": {
        "id": "68mfBQNZcAfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.self_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.self_dropout = layers.Dropout(0.5)\n",
        "        self.enc_dropout = layers.Dropout(0.1)\n",
        "        self.ffn_dropout = layers.Dropout(0.1)\n",
        "        self.ffn = keras.Sequential([\n",
        "            layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "\n",
        "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
        "        \"\"\"Gera a máscara de atenção causal para evitar a atenção em futuros tokens.\"\"\"\n",
        "        i = tf.range(n_dest)[:, None]\n",
        "        j = tf.range(n_src)\n",
        "        m = i >= j - n_src + n_dest\n",
        "        mask = tf.cast(m, dtype)\n",
        "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "        mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, enc_out, target):\n",
        "        input_shape = tf.shape(target)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        target_att = self.self_att(target, target, attention_mask=causal_mask)\n",
        "        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n",
        "        enc_out = self.enc_att(target_norm, enc_out)\n",
        "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n",
        "        ffn_out = self.ffn(enc_out_norm)\n",
        "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n",
        "        return ffn_out_norm\n"
      ],
      "metadata": {
        "id": "riUmt9Z3cDVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definição do Modelo Transformer Completo"
      ],
      "metadata": {
        "id": "J7BCCV8UcGwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(keras.Model):\n",
        "    def __init__(self, num_hid=64, num_head=2, num_feed_forward=128, source_maxlen=100,\n",
        "                 target_maxlen=100, num_layers_enc=4, num_layers_dec=1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n",
        "        self.num_layers_enc = num_layers_enc\n",
        "        self.num_layers_dec = num_layers_dec\n",
        "        self.target_maxlen = target_maxlen\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
        "        self.dec_input = TokenEmbedding(num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid)\n",
        "\n",
        "        self.encoder = keras.Sequential([self.enc_input] + [\n",
        "            TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
        "            for _ in range(num_layers_enc)\n",
        "        ])\n",
        "\n",
        "        for i in range(num_layers_dec):\n",
        "            setattr(self, f\"dec_layer_{i}\", TransformerDecoder(num_hid, num_head, num_feed_forward))\n",
        "\n",
        "        self.classifier = layers.Dense(num_classes)\n",
        "\n",
        "    def decode(self, enc_out, target):\n",
        "        y = self.dec_input(target)\n",
        "        for i in range(self.num_layers_dec):\n",
        "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n",
        "        return y\n",
        "\n",
        "    def call(self, inputs):\n",
        "        source = inputs[0]\n",
        "        target = inputs[1]\n",
        "        x = self.encoder(source)\n",
        "        y = self.decode(x, target)\n",
        "        return self.classifier(y)\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_metric]\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        \"\"\"Processa um batch dentro do model.fit().\"\"\"\n",
        "        source = batch[\"source\"]\n",
        "        target = batch[\"target\"]\n",
        "        dec_input = target[:, :-1]\n",
        "        dec_target = target[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            preds = self([source, dec_input])\n",
        "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
        "            loss = self.compute_loss(None, one_hot, preds, sample_weight=mask)\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\"loss\": self.loss_metric.result()}\n",
        "\n",
        "    def test_step(self, batch):\n",
        "        source = batch[\"source\"]\n",
        "        target = batch[\"target\"]\n",
        "        dec_input = target[:, :-1]\n",
        "        dec_target = target[:, 1:]\n",
        "        preds = self([source, dec_input])\n",
        "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
        "        loss = self.compute_loss(None, one_hot, preds, sample_weight=mask)\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\"loss\": self.loss_metric.result()}\n",
        "\n",
        "    def generate(self, source, target_start_token_idx):\n",
        "        \"\"\"Executa inferência usando decodificação greedy.\"\"\"\n",
        "        bs = tf.shape(source)[0]\n",
        "        enc = self.encoder(source)\n",
        "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
        "        dec_logits = []\n",
        "        for i in range(self.target_maxlen - 1):\n",
        "            dec_out = self.decode(enc, dec_input)\n",
        "            logits = self.classifier(dec_out)\n",
        "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n",
        "            dec_logits.append(last_logit)\n",
        "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
        "        return dec_input\n"
      ],
      "metadata": {
        "id": "ZMtabhV4cJlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregamento dos Dados"
      ],
      "metadata": {
        "id": "b5BBjWKRcMOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixar e extrair o conjunto de dados LJSpeech\n",
        "keras.utils.get_file(\n",
        "    os.path.join(os.getcwd(), \"data.tar.gz\"),\n",
        "    \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\",\n",
        "    extract=True,\n",
        "    archive_format=\"tar\",\n",
        "    cache_dir=\".\",\n",
        ")\n",
        "\n",
        "# Definir caminho dos dados\n",
        "saveto = \"./datasets/LJSpeech-1.1\"\n",
        "wavs = glob(\"{}/**/*.wav\".format(saveto), recursive=True)\n",
        "\n",
        "# Criar um mapeamento de IDs de áudio para textos\n",
        "id_to_text = {}\n",
        "with open(os.path.join(saveto, \"metadata.csv\"), encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        id = line.strip().split(\"|\")[0]\n",
        "        text = line.strip().split(\"|\")[2]\n",
        "        id_to_text[id] = text\n",
        "\n",
        "def get_data(wavs, id_to_text, maxlen=50):\n",
        "    \"\"\"Retorna o mapeamento de caminhos de áudio e textos de transcrição\"\"\"\n",
        "    data = []\n",
        "    for w in wavs:\n",
        "        id = w.split(\"/\")[-1].split(\".\")[0]\n",
        "        if len(id_to_text[id]) < maxlen:\n",
        "            data.append({\"audio\": w, \"text\": id_to_text[id]})\n",
        "    return data\n",
        "\n",
        "class VectorizeChar:\n",
        "    def __init__(self, max_len=50):\n",
        "        self.vocab = (\n",
        "            [\"-\", \"#\", \"<\", \">\"]\n",
        "            + [chr(i + 96) for i in range(1, 27)]\n",
        "            + [\" \", \".\", \",\", \"?\"]\n",
        "        )\n",
        "        self.max_len = max_len\n",
        "        self.char_to_idx = {ch: i for i, ch in enumerate(self.vocab)}\n",
        "\n",
        "    def __call__(self, text):\n",
        "        text = text.lower()\n",
        "        text = text[:self.max_len - 2]\n",
        "        text = \"<\" + text + \">\"\n",
        "        pad_len = self.max_len - len(text)\n",
        "        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n",
        "\n",
        "    def get_vocabulary(self):\n",
        "        return self.vocab\n",
        "\n",
        "# Preparar o dataset\n",
        "max_target_len = 200  # todas as transcrições no nosso dado são < 200 caracteres\n",
        "data = get_data(wavs, id_to_text, max_target_len)\n",
        "vectorizer = VectorizeChar(max_target_len)\n",
        "print(\"vocab size\", len(vectorizer.get_vocabulary()))\n",
        "\n",
        "def create_text_ds(data):\n",
        "    texts = [_[\"text\"] for _ in data]\n",
        "    text_ds = [vectorizer(t) for t in texts]\n",
        "    text_ds = tf.data.Dataset.from_tensor_slices(text_ds)\n",
        "    return text_ds\n",
        "\n",
        "def path_to_audio(path):\n",
        "    # Geração do espectrograma usando STFT\n",
        "    audio = tf.io.read_file(path)\n",
        "    audio, _ = tf.audio.decode_wav(audio, 1)\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\n",
        "    x = tf.math.pow(tf.abs(stfts), 0.5)\n",
        "    # Normalização\n",
        "    means = tf.math.reduce_mean(x, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(x, 1, keepdims=True)\n",
        "    x = (x - means) / stddevs\n",
        "    audio_len = tf.shape(x)[0]\n",
        "    # Padding para 10 segundos\n",
        "    pad_len = 2754\n",
        "    paddings = tf.constant([[0, pad_len], [0, 0]])\n",
        "    x = tf.pad(x, paddings, \"CONSTANT\")[:pad_len, :]\n",
        "    return x\n",
        "\n",
        "def create_audio_ds(data):\n",
        "    flist = [_[\"audio\"] for _ in data]\n",
        "    audio_ds = tf.data.Dataset.from_tensor_slices(flist)\n",
        "    audio_ds = audio_ds.map(path_to_audio, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return audio_ds\n",
        "\n",
        "def create_tf_dataset(data, bs=4):\n",
        "    audio_ds = create_audio_ds(data)\n",
        "    text_ds = create_text_ds(data)\n",
        "    ds = tf.data.Dataset.zip((audio_ds, text_ds))\n",
        "    ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\n",
        "    ds = ds.batch(bs)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# Dividir o dataset em treinamento e validação\n",
        "split = int(len(data) * 0.99)\n",
        "train_data = data[:split]\n",
        "test_data = data[split:]\n",
        "ds = create_tf_dataset(train_data, bs=64)\n",
        "val_ds = create_tf_dataset(test_data, bs=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfBGmTymcQh0",
        "outputId": "4a811b66-8cc7-405d-a6f3-c4e718216f3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
            "2748572632/2748572632 [==============================] - 45s 0us/step\n",
            "vocab size 34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funções de Treinamento e Geração"
      ],
      "metadata": {
        "id": "B9oY8l-XcTMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DisplayOutputs(keras.callbacks.Callback):\n",
        "    def __init__(self, batch, idx_to_token, target_start_token_idx=27, target_end_token_idx=28):\n",
        "        \"\"\"Exibe um batch de saídas após cada época\"\"\"\n",
        "        self.batch = batch\n",
        "        self.target_start_token_idx = target_start_token_idx\n",
        "        self.target_end_token_idx = target_end_token_idx\n",
        "        self.idx_to_char = idx_to_token\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch % 5 != 0:\n",
        "            return\n",
        "        source = self.batch[\"source\"]\n",
        "        target = self.batch[\"target\"].numpy()\n",
        "        bs = tf.shape(source)[0]\n",
        "        preds = self.model.generate(source, self.target_start_token_idx)\n",
        "        preds = preds.numpy()\n",
        "        for i in range(bs):\n",
        "            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n",
        "            prediction = \"\"\n",
        "            for idx in preds[i, :]:\n",
        "                prediction += self.idx_to_char[idx]\n",
        "                if idx == self.target_end_token_idx:\n",
        "                    break\n",
        "            print(f\"target:     {target_text.replace('-','')}\")\n",
        "            print(f\"prediction: {prediction}\\n\")\n",
        "\n",
        "class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, init_lr=0.00001, lr_after_warmup=0.001, final_lr=0.00001,\n",
        "                 warmup_epochs=15, decay_epochs=85, steps_per_epoch=203):\n",
        "        super().__init__()\n",
        "        self.init_lr = init_lr\n",
        "        self.lr_after_warmup = lr_after_warmup\n",
        "        self.final_lr = final_lr\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.decay_epochs = decay_epochs\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "    def calculate_lr(self, epoch):\n",
        "        \"\"\"Aquecimento linear - decaimento linear\"\"\"\n",
        "        warmup_lr = (\n",
        "            self.init_lr\n",
        "            + ((self.lr_after_warmup - self.init_lr) / (self.warmup_epochs - 1)) * epoch\n",
        "        )\n",
        "        decay_lr = tf.math.maximum(\n",
        "            self.final_lr,\n",
        "            self.lr_after_warmup\n",
        "            - (epoch - self.warmup_epochs)\n",
        "            * (self.lr_after_warmup - self.final_lr)\n",
        "            / self.decay_epochs,\n",
        "        )\n",
        "        return tf.math.minimum(warmup_lr, decay_lr)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        epoch = step // self.steps_per_epoch\n",
        "        epoch = tf.cast(epoch, \"float32\")\n",
        "        return self.calculate_lr(epoch)\n",
        "\n",
        "batch = next(iter(val_ds))\n",
        "\n",
        "# O vocabulário para converter índices previstos em caracteres\n",
        "idx_to_char = vectorizer.get_vocabulary()\n",
        "display_cb = DisplayOutputs(\n",
        "    batch, idx_to_char, target_start_token_idx=2, target_end_token_idx=3\n",
        ")  # Definir os argumentos conforme o índice de vocabulário para '<' e '>'\n",
        "\n",
        "model = Transformer(\n",
        "    num_hid=200,\n",
        "    num_head=2,\n",
        "    num_feed_forward=400,\n",
        "    target_maxlen=max_target_len,\n",
        "    num_layers_enc=4,\n",
        "    num_layers_dec=1,\n",
        "    num_classes=34,\n",
        ")\n",
        "loss_fn = keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    label_smoothing=0.1,\n",
        ")\n",
        "\n",
        "learning_rate = CustomSchedule(\n",
        "    init_lr=0.00001,\n",
        "    lr_after_warmup=0.001,\n",
        "    final_lr=0.00001,\n",
        "    warmup_epochs=15,\n",
        "    decay_epochs=85,\n",
        "    steps_per_epoch=len(ds),\n",
        ")\n",
        "optimizer = keras.optimizers.Adam(learning_rate)\n",
        "model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "history = model.fit(ds, validation_data=val_ds, callbacks=[display_cb], epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3093HSQYcX68",
        "outputId": "4a6ce1a4-7baa-4361-b819-59b1ef142a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "203/203 [==============================] - ETA: 0s - loss: 1.7237target:     <immediately after president kennedy#s stretcher was wheeled into trauma room one, secret service agents took positions at the door of the small emergency room.>\n",
            "prediction: <a t iale s the in t f tit t at s t a oe a st nt t so t there n ooe ie t t t tioas t st t ie t f s tis on ithe the the tie the t the t til t t ie t the t therie s th t tits t te t tt tthiae t in ioea \n",
            "\n",
            "target:     <and how, in punishment for all his wickedness, he became a calf, and for seven years grazed the grass in the fields about the city.>\n",
            "prediction: <a t iale s the in t f tit t at s t a oe a st nt t so t there n ooe ie t t t tioas t st t ie t f s tis on ithe the the tie the t the t til t t ie t the t therie s th t tits t te t tt tthiae t in ioea \n",
            "\n",
            "target:     <that the jails attached to corporate jurisdictions continue to be the fruitful sources>\n",
            "prediction: <a t iale s the in t f tit t at s t a oe a st nt t so t there n ooe ie t t t tioas t st t ie t f s tis on ithe the the tie the t the t til t t ie t the t therie s th t tits t te t tt tthiae t in ioea \n",
            "\n",
            "target:     <under superior orders all the doors and gates of this block were left open at night, to allow the night watchman to pass freely to all parts.>\n",
            "prediction: <a t iale s the in t f tit t at s t a oe a st nt t so t there n ooe ie t t t tioas t st t ie t f s tis on ithe the the tie the t the t til t t ie t the t therie s th t tits t te t tt tthiae t in ioea \n",
            "\n",
            "203/203 [==============================] - 151s 643ms/step - loss: 1.7237 - val_loss: 1.5876\n",
            "Epoch 2/10\n",
            "203/203 [==============================] - 123s 602ms/step - loss: 1.3909 - val_loss: 1.3805\n",
            "Epoch 3/10\n",
            "203/203 [==============================] - 123s 605ms/step - loss: 1.3205 - val_loss: 1.3527\n",
            "Epoch 4/10\n",
            "203/203 [==============================] - 122s 602ms/step - loss: 1.3060 - val_loss: 1.3434\n",
            "Epoch 5/10\n",
            "203/203 [==============================] - 122s 601ms/step - loss: 1.2962 - val_loss: 1.3326\n",
            "Epoch 6/10\n",
            "203/203 [==============================] - ETA: 0s - loss: 1.2758target:     <immediately after president kennedy#s stretcher was wheeled into trauma room one, secret service agents took positions at the door of the small emergency room.>\n",
            "prediction: <whe was the the the stre the se the the se tre tre the sthe the sise the ase the tre the tre the sthe tre the tre the sthe tre of of the the trere ofthe tre oftrofthe.>\n",
            "\n",
            "target:     <and how, in punishment for all his wickedness, he became a calf, and for seven years grazed the grass in the fields about the city.>\n",
            "prediction: <and athe the sse the an the ane the the assin the an the the an an an the the the the ssissofofofofofofof the the the s the the fofof the the.>\n",
            "\n",
            "target:     <that the jails attached to corporate jurisdictions continue to be the fruitful sources>\n",
            "prediction: <the the sthe the the sthe the the she the the fofre the she f the the the the frsthe the the the shere there s.>\n",
            "\n",
            "target:     <under superior orders all the doors and gates of this block were left open at night, to allow the night watchman to pass freely to all parts.>\n",
            "prediction: <on the the the the the the the the the ware the the the sso the the the the the the the the the the the the the the sofre the the ofre sofre the ofre the s.>\n",
            "\n",
            "203/203 [==============================] - 131s 642ms/step - loss: 1.2758 - val_loss: 1.3009\n",
            "Epoch 7/10\n",
            "203/203 [==============================] - 127s 623ms/step - loss: 1.2351 - val_loss: 1.2568\n",
            "Epoch 8/10\n",
            "203/203 [==============================] - 123s 604ms/step - loss: 1.1572 - val_loss: 1.1373\n",
            "Epoch 9/10\n",
            "203/203 [==============================] - 121s 594ms/step - loss: 1.0068 - val_loss: 0.9692\n",
            "Epoch 10/10\n",
            "203/203 [==============================] - 120s 590ms/step - loss: 0.8603 - val_loss: 0.8630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Fo1k8ANud58I"
      }
    }
  ]
}